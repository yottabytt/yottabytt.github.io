<!DOCTYPE html>
<html lang="en-us">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">



<meta property="twitter:title" content="Topic Bytt #2 - How entropy got its formula? (Part I)">

    <meta property="twitter:card" content="summary">

<meta property="twitter:description" content="">

<title>


     yottabytt - Topic Bytt #2 - How entropy got its formula? (Part I) 

</title>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>



<link rel="canonical" href="https://yottabytt.com/bytts/topic/entropy/">


<link rel="stylesheet" href="https://yottabytt.com/css/main.v1.0.0.css">








<link rel="shortcut icon"

    href="https://yottabytt.com/images/YB.png"

>





</head>



<body>


<section class="header">

    <div class="container">
        <a href="https://yottabytt.com/"><img class="logo" src="https://yottabytt.com/images/YBWhite.png" alt="logo" /></a>
        <div class="content">
            
            <nav>
                <ul>
                    
                    
                        
                            
                            <a href="https://yottabytt.com/bytts/"><li>bytts</li></a>
                            
                        
                    
                    
                        
                            <a href="https://yottabytt.com/about/"><li>About</li></a>
                        
                    
                        
                    
                </ul>
            </nav>
        </div>
    </div>
</section>

<section class="main">
    <div class="container">
        <div class="content">
            <div class="page-heading">

    Topic Bytt #2 - How entropy got its formula? (Part I)

</div>

            <div class="markdown">
                
    <p>Cross-entropy is a measure that pops up at many places within machine learning.
So, we should be aware of the the connection between entropy (Information theory) and product-rule (Probability theory).</p>

<p>Let \(X\) be a discrete random variable and \(x_1, x_2, &hellip;,x_m\) be the values (events) that \(X\) can take.</p>

<p>Let&rsquo;s look at a case of observing any \(x_i\).</p>

<p>We are told that the probability of observing \(x_1\) is \(0.01\). Whereas, other \(x_j\) have high probabilities
in the range \([0.2, 0.5]\).</p>

<p>Assume we are in trial number \(3\). With the given information \(p(x_1)\), we expect that \(x_1\) might not be occuring. But to our surpise, it has occured.</p>

<p>\(h(x)\) is a function that is now brought into picture to handle such surprises (information gains).</p>

<p>Let&rsquo;s say such a surprise has occured for two independent events \(X=x_1\) and \(X=x_2\). So, total gain \(h(x_1, x_2) = h(x_1) + h(x_2)\). By product-rule, we know that \(p(x_1, x_2) = p(x_1).p(x_2)\).</p>

<p>As we might be knowing only \(p(x_i, x_j)\), we shall try to represent \(h(x_i, x_j)\) in terms of \(p(x_i, x_j)\).</p>

<p>Think of some function \(f(x)\) such that, \(h(x_i, x_j) = f(p(x_i, x_j))\).</p>

<p>Hint: \(log_b(u.v) = log_b(u) + log_b(v)\)</p>

<p>So, something like the following may be good,<br />
\(h(x_i, x_j) = log_b(p(x_i, x_j)) = log_b(p(x_i)) + log_b(p(x_j))\).</p>

<p>Now, we shall just consider a single event \(x\), \(h(x) = log_b(p(x))\).</p>

<p>Hold on, we know gains can be high when we observe a highly unlikely event like \(x_1\).
Does, current \(h(x)\) capture it?</p>

<p>\(h(x_1) = log_b(0.01) = -2 \), when \(b=10\)</p>

<p>A gain represented by a positive number could look neat. So, why not<br />
\(h(x) = - log_b(p(x))\)?</p>

<p>It doesn&rsquo;t affect any of the preconditions. Thus, we have reached close to the actual formula of entropy.</p>


            </div>
        </div>
    </div>
</section>
<section class="footer">
    <div class="container">
        <div class="copyright">

        

        </div>
        <div class="icons">

        
            <a href="https://stackexchange.com/users/4093868/yottabytt?tab=accounts" target="_blank">
                <img class="icon" src="/img/stackexchange.svg" alt="stackexchange" />
            </a>
        

        
            <a href="https://github.com/yottabytt" target="_blank">
                <img class="icon" src="/img/github.svg" alt="github" />
            </a>
        

        

        

        
            <a href="mailto:yottabytt@gmail.com">
                <img class="icon" src="/img/email.svg" alt="email" />
            </a>
        

        
            <a href="https://yottabytt.com/index.xml">
                <img class="icon" src="/img/rss.svg" alt="rss" />
            </a>
        

        </div>
    </div>
</section>







<script>
  window.onload = function() {
    
    
  };
</script>

<link href="https://fonts.googleapis.com/css?family=Raleway:400,600,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,700,700i" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700" rel="stylesheet">


</body>
</html>

